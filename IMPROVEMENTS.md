# GitHub Data Pipeline - Improvements Summary

## Changes Made (2025-12-06)

### ‚úÖ 1. REQUIRED_FIELDS Moved to Top of File

**Location:** Lines 47-69 in [src/extract_github_data.py](src/extract_github_data.py:47-69)

**Why:** For better visibility and editability. You can now easily customize which fields are required without searching through the code.

**How to Use:**
```python
# Edit the REQUIRED_FIELDS list at top of file
REQUIRED_FIELDS = [
    'id',
    'name',
    'full_name',
    'owner',
    'html_url',
    # Uncomment additional fields as needed:
    # 'stargazers_count',
    # 'language',
    # 'created_at',
]
```

**Analysis Questions Supported:**
- **Total repositories on GitHub**: Requires `id`
- **Most-starred repos**: Requires `stargazers_count`, `name`, `full_name`
- **Popular repos by language**: Requires `language`, `stargazers_count`
- **User/org repo count**: Requires `owner`
- **Trending by creation date**: Requires `created_at`
- **Language popularity over time**: Requires `language`, `created_at`

---

### ‚úÖ 2. Validation Logic Now Filters Invalid Repos

**What Changed:**
- Added new `filter_valid_repositories()` function
- Invalid repos are now **excluded** from the final dataset
- Only valid repos are uploaded to S3
- Only first 5 invalid repos are logged (to avoid log spam)

**Benefits:**
- Cleaner data in S3
- Better data quality for Snowflake
- Reduced storage costs (no invalid data)
- Analysis queries won't fail on missing fields

**Example Output:**
```
INFO - Validation complete: 200/200 repositories valid
INFO - Filtered out: 0 invalid repositories
INFO - Validation rate: 100.00%
```

---

### ‚úÖ 3. S3 Folder Structure with Date Partitioning

**New S3 Structure:**
```
s3://github-api0-upload/
‚îú‚îÄ‚îÄ 2025/
‚îÇ   ‚îî‚îÄ‚îÄ 12/
‚îÇ       ‚îî‚îÄ‚îÄ 06/
‚îÇ           ‚îî‚îÄ‚îÄ github_repos_2025-12-06_13-50-39.json
```

**Benefits:**
- **Better organization**: Files grouped by date
- **Easier queries**: Snowflake can partition by date folder
- **Faster searches**: Find files by date without scanning all files
- **Cost optimization**: Query only specific date ranges

**Configuration:**
```bash
# In .env file:
S3_USE_DATE_PARTITIONING=true   # Use yyyy/mm/dd/ folders
S3_USE_DATE_PARTITIONING=false  # Upload to root (old behavior)
```

---

### ‚úÖ 4. Intelligent Page Loading (Incremental Caching)

**How It Works:**
- Script checks cache for each page before making API call
- If page exists in cache, it's loaded instantly (no API call)
- Only uncached pages trigger API requests

**Test Results:**
| Pages Requested | Cached | API Calls | Repositories |
|-----------------|--------|-----------|--------------|
| 1               | 1      | 0         | 100          |
| 2               | 2      | 0         | 200          |
| 5               | 2      | 3         | 500          |
| 10              | 5      | 5         | 1000         |

**Usage:**
```bash
# First run: Load 2 pages (2 API calls)
export PAGES_TO_EXTRACT=2
python src/extract_github_data.py

# Second run: Load 5 pages (only 3 NEW API calls)
export PAGES_TO_EXTRACT=5
python src/extract_github_data.py --use-cache

# Third run: Load 10 pages (only 5 NEW API calls)
export PAGES_TO_EXTRACT=10
python src/extract_github_data.py --use-cache
```

---

### ‚úÖ 5. Log Files - New Log Per Run

**Answer:** YES, a new log file is created on each run.

**Log File Naming:**
```
logs/github_extract_YYYYMMDD_HHMMSS.log
```

**Examples:**
```
logs/github_extract_20251206_131636.log  # First run
logs/github_extract_20251206_134906.log  # Second run
logs/github_extract_20251206_135039.log  # Third run
```

**Why:** This allows you to:
- Track each extraction separately
- Debug issues by comparing logs
- Audit extraction history
- Keep logs organized by time

---

## ‚ùì Why Git Push Didn't Run

Git push requires authentication, which cannot be automated in this environment.

**What was done:**
‚úÖ Created feature branch
‚úÖ Committed changes
‚úÖ Merged to local develop

**What you need to do:**
```bash
git push origin develop
```

You'll be prompted for GitHub credentials or need to set up SSH/token authentication.

---

## üöÄ Testing Incremental Page Loading

### Test 1: Load 1 Page
```bash
export PAGES_TO_EXTRACT=1
python src/extract_github_data.py --use-cache --skip-upload
```
**Result:** 0 API calls (page 1 cached), 100 repos

### Test 2: Load 2 Pages
```bash
export PAGES_TO_EXTRACT=2
python src/extract_github_data.py --use-cache --skip-upload
```
**Result:** 0 API calls (pages 1-2 cached), 200 repos

### Test 3: Load 5 Pages
```bash
export PAGES_TO_EXTRACT=5
python src/extract_github_data.py --use-cache --skip-upload
```
**Result:** 3 API calls (pages 3-5 fetched), 500 repos

### Test 4: Load 10 Pages
```bash
export PAGES_TO_EXTRACT=10
python src/extract_github_data.py --use-cache --skip-upload
```
**Result:** 5 API calls (pages 6-10 fetched), 1000 repos

### Load All Pages Without Reloading
```bash
# Start small, then increase - cached pages won't reload
PAGES_TO_EXTRACT=1 python src/extract_github_data.py --use-cache --skip-upload
PAGES_TO_EXTRACT=2 python src/extract_github_data.py --use-cache --skip-upload
PAGES_TO_EXTRACT=5 python src/extract_github_data.py --use-cache --skip-upload
PAGES_TO_EXTRACT=10 python src/extract_github_data.py --use-cache --skip-upload
PAGES_TO_EXTRACT=25 python src/extract_github_data.py --use-cache --skip-upload
```

Each run only fetches NEW pages!

---

## üìä API Token Usage Optimization

### With Intelligent Caching:
1. **First run (2 pages):** 2 API calls
2. **Second run (5 pages):** 3 API calls (reuses cached pages 1-2)
3. **Third run (10 pages):** 5 API calls (reuses cached pages 1-5)
4. **Total:** 10 API calls instead of 17 (saved 7 calls!)

### Without Cache:
- Every run makes ALL API calls from scratch
- 2 + 5 + 10 = 17 API calls total

### Savings:
- **41% reduction** in API calls with caching
- Perfect for development/testing
- Stays well within rate limits

---

## üîß Configuration Options

### Environment Variables
```bash
# Extraction
export PAGES_TO_EXTRACT=10        # Number of pages to extract
export REPOS_PER_PAGE=50           # Repos per page (max 100)

# S3
export S3_USE_DATE_PARTITIONING=true   # Enable yyyy/mm/dd folders
export AWS_S3_BUCKET=my-bucket         # S3 bucket name
export AWS_REGION=us-east-2            # AWS region

# Caching
export USE_CACHE=true              # Enable smart caching
export CACHE_DIR=cache             # Cache directory

# Logging
export LOG_LEVEL=INFO              # Log level (DEBUG, INFO, WARNING, ERROR)
export LOG_DIR=logs                # Logs directory
```

---

## üìà Production Usage

### Daily Incremental Load
```bash
# Day 1: Load 1000 repos
PAGES_TO_EXTRACT=10 python src/extract_github_data.py

# Day 2: Load next 1000 repos (cache prevents re-fetching Day 1 data)
PAGES_TO_EXTRACT=20 python src/extract_github_data.py --use-cache

# Day 3: Load next 1000 repos
PAGES_TO_EXTRACT=30 python src/extract_github_data.py --use-cache
```

Each day only fetches NEW pages!

---

## üéØ Next Steps

1. **Push to remote develop:**
   ```bash
   git push origin develop
   ```

2. **Adjust REQUIRED_FIELDS** for your analysis needs (see top of script)

3. **Scale up extraction:**
   ```bash
   PAGES_TO_EXTRACT=100 python src/extract_github_data.py
   ```

4. **Set up daily scheduling** (cron or AWS Lambda)

5. **Implement Snowflake integration** for automated data loading

---

## üìù Files Modified

1. [src/extract_github_data.py](src/extract_github_data.py) - Main script with all improvements
2. [.env.example](.env.example) - Updated with new S3_USE_DATE_PARTITIONING setting
3. [IMPROVEMENTS.md](IMPROVEMENTS.md) - This file

---

## ‚úÖ Summary

All your requested improvements are complete and tested:

1. ‚úÖ REQUIRED_FIELDS moved to top (lines 47-69)
2. ‚úÖ Invalid repos are filtered out
3. ‚úÖ S3 uses yyyy/mm/dd folder structure
4. ‚úÖ Intelligent page caching prevents reloading
5. ‚úÖ New log file per run with timestamps
6. ‚úÖ Tested with 1, 2, 5, 10 pages successfully

**Ready for production!** üöÄ
